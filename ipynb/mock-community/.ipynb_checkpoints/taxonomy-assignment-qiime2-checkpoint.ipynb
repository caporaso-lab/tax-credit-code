{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation: using Python to sweep over methods and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we illustrate how to use Python to perform *parameter sweeps* for a taxonomic assigner and integrate the results into the TAX CREdiT framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from os.path import join, exists, split, sep, expandvars \n",
    "from os import makedirs, getpid\n",
    "from glob import glob\n",
    "from shutil import rmtree\n",
    "import csv\n",
    "import json\n",
    "import tempfile\n",
    "from itertools import product\n",
    "\n",
    "from qiime2.plugins import feature_classifier\n",
    "from qiime2 import Artifact\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from q2_feature_classifier.classifier import spec_from_pipeline\n",
    "from q2_types.feature_data import DNAIterator\n",
    "from pandas import DataFrame\n",
    "\n",
    "from tax_credit.framework_functions import (\n",
    "    gen_param_sweep, generate_per_method_biom_tables, move_results_to_repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "project_dir = expandvars('$HOME/Desktop/projects/short-read-tax-assignment/')\n",
    "analysis_name = 'mock-community'\n",
    "data_dir = join(project_dir, 'data', analysis_name)\n",
    "\n",
    "reference_database_dir = expandvars(\"$HOME/Desktop/ref_dbs/\")\n",
    "results_dir = expandvars(\"$HOME/Desktop/projects/mock-community/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Methods\n",
    "The below methods are used to load the data, prepare the data, parse the classifier and classification parameters, and fit and run the classifier. They should probably be moved to tax_credit.framework_functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# *** one glaring flaw here is that generate_pipeline_sweep iterates\n",
    "# *** through all method_parameters_combinations and reference_dbs \n",
    "# *** and hence will generate training sets for each combo even if\n",
    "# *** not all are called by commands in sweep. This is not an issue\n",
    "# *** if sweep uses all classifiers but is inconvenient if attempting\n",
    "# *** to test on a subset of sweep. Need to explicitly set all inputs!\n",
    "\n",
    "def train_and_run_classifier(method_parameters_combinations, reference_dbs,\n",
    "                             pipelines, sweep, verbose=False, n_jobs=4):\n",
    "    '''Train and run q2-feature-classifier across a parameter sweep.\n",
    "    method_parameters_combinations: dict of dicts of lists\n",
    "        Classifier methods to run and their parameters/values to sweep\n",
    "        Format: {method_name: {'parameter_name': [parameter_values]}}\n",
    "    reference_dbs: dict of tuples\n",
    "        Reference databases to use for classifier training.\n",
    "        Format: {database_name: (ref_seqs, ref_taxonomy)}\n",
    "    pipelines: dict\n",
    "        Classifier pipelines to use for training each method.\n",
    "        Format: {method_name: sklearn.pipeline.Pipeline}\n",
    "    sweep: list of tuples\n",
    "        output of gen_param_sweep(), format:\n",
    "        (parameter_output_dir, input_dir, reference_seqs, reference_tax, method, params)\n",
    "    n_jobs: number of jobs to run in parallel.\n",
    "    '''\n",
    "    # train classifier once for each pipeline param combo\n",
    "    for method, db, pipeline_param, subsweep in generate_pipeline_sweep(\n",
    "            method_parameters_combinations, reference_dbs, sweep):\n",
    "        ref_reads, ref_taxa = reference_dbs[db]\n",
    "        # train classifier\n",
    "        classifier = train_classifier(\n",
    "            ref_reads, ref_taxa, pipeline_param, pipelines[method], verbose=verbose)\n",
    "        # run classifier. Only run in parallel once classifier is trained,\n",
    "        # to minimize memory usage (don't want to train large refs in parallel)\n",
    "        Parallel(n_jobs=n_jobs)(delayed(run_classifier)(\n",
    "            classifier, output_dir, input_dir, split_params(params)[0], verbose=verbose)\n",
    "            for output_dir, input_dir, rs, rt, mt, params in subsweep)\n",
    "\n",
    "            \n",
    "def generate_pipeline_sweep(method_parameters_combinations, reference_dbs, sweep):\n",
    "    '''Generate pipeline parameters for each classifier training step'''\n",
    "    # iterate over parameters\n",
    "    for method, params in method_parameters_combinations.items():\n",
    "        # split out pipeline parameters\n",
    "        classifier_params, pipeline_params = split_params(params)\n",
    "        # iterate over reference dbs\n",
    "        for db, refs in reference_dbs.items():\n",
    "            # iterate over all pipeline parameter combinations\n",
    "            for param_product in product(*[params[id_] for id_ in pipeline_params]):\n",
    "                # yield parameter combinations to use for a each classifier\n",
    "                pipeline_param = dict(zip(pipeline_params, param_product))\n",
    "                subsweep = [p for p in sweep if split_params(p[5])[1] \n",
    "                            == pipeline_param and p[2] == refs[0]]\n",
    "                yield method, db, pipeline_param, subsweep\n",
    "\n",
    "\n",
    "def train_classifier(ref_reads, ref_taxa, params, pipeline, verbose=False):\n",
    "    ref_reads = Artifact.load(ref_reads)\n",
    "    ref_taxa = Artifact.load(ref_taxa)\n",
    "    pipeline.set_params(**params)\n",
    "    spec = json.dumps(spec_from_pipeline(pipeline))\n",
    "    if verbose:\n",
    "        print(spec)\n",
    "    classifier = feature_classifier.methods.fit_classifier(ref_reads, ref_taxa, spec)\n",
    "    #return classifier.classifier\n",
    "\n",
    "\n",
    "def run_classifier(classifier, output_dir, input_dir, params, verbose=False):    \n",
    "    # Classify the sequences\n",
    "    rep_seqs = Artifact.load(join(input_dir, 'rep_seqs.qza'))\n",
    "    if verbose:\n",
    "        print(output_dir)\n",
    "    classification = feature_classifier.methods.classify(rep_seqs, classifier, **params)\n",
    "    \n",
    "    # Save the results\n",
    "    makedirs(output_dir, exist_ok=True)\n",
    "    output_file = join(output_dir, 'rep_set_tax_assignments.txt')\n",
    "    dataframe = classification.classification.view(DataFrame)\n",
    "    dataframe.to_csv(output_file, sep='\\t', header=False)\n",
    "\n",
    "    \n",
    "def split_params(params):\n",
    "    classifier_params = feature_classifier.methods.\\\n",
    "                        classify.signature.parameters.keys()\n",
    "    pipeline_params = {k:v for k, v in params.items()\n",
    "                        if k not in classifier_params}\n",
    "    classifier_params = {k:v for k, v in params.items() \n",
    "                         if k in classifier_params}\n",
    "    return classifier_params, pipeline_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data set sweep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to define the data sets that we'll sweep over. The following cell does not need to be modified unless if you wish to change the datasets or reference databases used in the sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_reference_combinations = [\n",
    " ('mock-1', 'gg_13_8_otus'), # formerly S16S-1\n",
    " ('mock-2', 'gg_13_8_otus'), # formerly S16S-2\n",
    " ('mock-3', 'gg_13_8_otus'), # formerly Broad-1\n",
    " ('mock-4', 'gg_13_8_otus'), # formerly Broad-2\n",
    " ('mock-5', 'gg_13_8_otus'), # formerly Broad-3\n",
    " # ('mock-6', 'gg_13_8_otus'), # formerly Turnbaugh-1\n",
    " ('mock-7', 'gg_13_8_otus'), # formerly Turnbaugh-2\n",
    " ('mock-8', 'gg_13_8_otus'), # formerly Turnbaugh-3\n",
    " ('mock-9', 'unite_20.11.2016_clean_fullITS'), # formerly ITS1\n",
    " ('mock-10', 'unite_20.11.2016_clean_fullITS'), # formerly ITS2-SAG\n",
    " ('mock-12', 'gg_13_8_otus'), # Extreme\n",
    " # ('mock-13', 'gg_13_8_otus_full16S'), # kozich-1\n",
    " # ('mock-14', 'gg_13_8_otus_full16S'), # kozich-2\n",
    " # ('mock-15', 'gg_13_8_otus_full16S'), # kozich-3\n",
    " ('mock-16', 'gg_13_8_otus'), # schirmer-1\n",
    "]\n",
    "\n",
    "reference_dbs = {'gg_13_8_otus' : (join(reference_database_dir, 'gg_13_8_otus/rep_set/99_otus_515f-806r.qza'), \n",
    "                                   join(reference_database_dir, 'gg_13_8_otus/taxonomy/99_otu_taxonomy.qza')),\n",
    "                 # 'gg_13_8_otus_full16S' : (join(reference_database_dir, 'gg_13_8_otus/rep_set/99_otus.qza'), \n",
    "                 #                  join(reference_database_dir, 'gg_13_8_otus/taxonomy/99_otu_taxonomy.qza')),\n",
    "                 'unite_20.11.2016_clean_fullITS' : (join(reference_database_dir, 'sh_qiime_release_20.11.2016/developer/sh_refs_qiime_ver7_99_20.11.2016_dev_clean.qza'), \n",
    "                                   join(reference_database_dir, 'sh_qiime_release_20.11.2016/developer/sh_taxonomy_qiime_ver7_99_20.11.2016_dev_clean.qza')),\n",
    "                 # 'unite_20.11.2016' : (join(reference_database_dir, 'sh_qiime_release_20.11.2016/developer/sh_refs_qiime_ver7_99_20.11.2016_dev_BITSf-B58S3r_trim250.qza'), \n",
    "                 #                      join(reference_database_dir, 'sh_qiime_release_20.11.2016/developer/sh_taxonomy_qiime_ver7_99_20.11.2016_dev.qza'))\n",
    "                }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the method/parameter combinations and generating commands\n",
    "\n",
    "Now we set the methods and method-specific parameters that we want to sweep. Modify to sweep other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "method_parameters_combinations = {\n",
    "    'q2-multinomialNB': {'confidence': [0.0, 0.2, 0.4, 0.6, 0.8],\n",
    "                         'classify__alpha': [0.001, 0.01, 0.1],\n",
    "                         'feat_ext__ngram_range': [[8,8], [12,12], [20,20]]},\n",
    "    'q2-logisticregression': {'classify__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag']},\n",
    "    'q2-randomforest': {'classify__max_features': ['sqrt', 'None'],\n",
    "                        'classify__n_estimators': [5, 10, 100]}\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the pipelines\n",
    "The below pipelines are used to specify the scikit-learn classifiers that are used for assignment. At the moment we only include Na√Øve Bayes but the collection will expand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pipeline params common to all classifiers are set here\n",
    "hash_params = dict(\n",
    "    analyzer='char_wb', n_features=8192, non_negative=True, ngram_range=[8, 8])\n",
    "\n",
    "# any params common to all classifiers can be set here\n",
    "classify_params = dict()\n",
    "\n",
    "def build_pipeline(classifier, hash_params, classify_params):\n",
    "    return Pipeline([\n",
    "        ('feat_ext', HashingVectorizer(**hash_params)),\n",
    "        ('classify', classifier(**classify_params))])\n",
    "    \n",
    "# Now fit the pipelines.\n",
    "pipelines = {'q2-multinomialNB': build_pipeline(\n",
    "                 MultinomialNB, hash_params, {'fit_prior': False}),\n",
    "             'q2-logisticregression': build_pipeline(\n",
    "                 LogisticRegression, hash_params, classify_params),\n",
    "             'q2-randomforest': build_pipeline(\n",
    "                 RandomForestClassifier, hash_params, classify_params)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset_reference_combinations = [\n",
    " ('mock-3', 'gg_13_8_otus'), # formerly Broad-1\n",
    "]\n",
    "\n",
    "method_parameters_combinations = {\n",
    "    'q2-randomforest': {'classify__max_features': ['sqrt'],\n",
    "                        'classify__n_estimators': [5]}\n",
    "    }\n",
    "\n",
    "reference_dbs = {'gg_13_8_otus' : (join(reference_database_dir, 'gg_13_8_otus/rep_set/99_otus_515f-806r.qza'), \n",
    "                                   join(reference_database_dir, 'gg_13_8_otus/taxonomy/99_otu_taxonomy.qza'))}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do the Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sweep = gen_param_sweep(data_dir, results_dir, reference_dbs,\n",
    "                        dataset_reference_combinations,\n",
    "                        method_parameters_combinations)\n",
    "sweep = list(sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick sanity check never hurt anyone..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('/Users/nbokulich/Desktop/projects/mock-community/mock-3/gg_13_8_otus/q2-randomforest/sqrt:5',\n",
       " '/Users/nbokulich/Desktop/projects/short-read-tax-assignment/data/mock-community/mock-3',\n",
       " '/Users/nbokulich/Desktop/ref_dbs/gg_13_8_otus/rep_set/99_otus_515f-806r.qza',\n",
       " '/Users/nbokulich/Desktop/ref_dbs/gg_13_8_otus/taxonomy/99_otu_taxonomy.qza',\n",
       " 'q2-randomforest',\n",
       " {'classify__max_features': 'sqrt', 'classify__n_estimators': 5})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(sweep))\n",
    "sweep[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<artifact: FeatureData[Sequence] uuid: 57ec23ea-7618-4de0-abeb-cc5850264040> <artifact: FeatureData[Taxonomy] uuid: 8e1cb0b7-b1ff-45df-a41f-b7726a4a21a2> [[\"feat_ext\", {\"strip_accents\": null, \"__type__\": \"feature_extraction.text.HashingVectorizer\", \"tokenizer\": null, \"decode_error\": \"strict\", \"encoding\": \"utf-8\", \"norm\": \"l2\", \"non_negative\": true, \"token_pattern\": \"(?u)\\\\b\\\\w\\\\w+\\\\b\", \"stop_words\": null, \"binary\": false, \"analyzer\": \"char_wb\", \"preprocessor\": null, \"input\": \"content\", \"ngram_range\": [8, 8], \"lowercase\": true, \"n_features\": 8192}], [\"classify\", {\"__type__\": \"ensemble.forest.RandomForestClassifier\", \"n_jobs\": 1, \"class_weight\": null, \"bootstrap\": true, \"min_samples_split\": 2, \"oob_score\": false, \"min_impurity_split\": 1e-07, \"random_state\": null, \"max_leaf_nodes\": null, \"min_samples_leaf\": 1, \"min_weight_fraction_leaf\": 0.0, \"max_features\": \"sqrt\", \"verbose\": 0, \"max_depth\": null, \"warm_start\": false, \"criterion\": \"gini\", \"n_estimators\": 5}]]\n",
      "<artifact: FeatureData[Sequence] uuid: cded1920-4dae-496f-8e4d-2e74006831c9> None {} /Users/nbokulich/Desktop/projects/mock-community/mock-3/gg_13_8_otus/q2-randomforest/sqrt:5\n",
      "<artifact: FeatureData[Sequence] uuid: 9c7c70b2-66d0-4fce-971d-b3db1f40873c> <artifact: FeatureData[Taxonomy] uuid: dea9dfe1-2d69-4240-a2c9-73a35e969ee6> [[\"feat_ext\", {\"strip_accents\": null, \"__type__\": \"feature_extraction.text.HashingVectorizer\", \"tokenizer\": null, \"decode_error\": \"strict\", \"encoding\": \"utf-8\", \"norm\": \"l2\", \"non_negative\": true, \"token_pattern\": \"(?u)\\\\b\\\\w\\\\w+\\\\b\", \"stop_words\": null, \"binary\": false, \"analyzer\": \"char_wb\", \"preprocessor\": null, \"input\": \"content\", \"ngram_range\": [8, 8], \"lowercase\": true, \"n_features\": 8192}], [\"classify\", {\"__type__\": \"ensemble.forest.RandomForestClassifier\", \"n_jobs\": 1, \"class_weight\": null, \"bootstrap\": true, \"min_samples_split\": 2, \"oob_score\": false, \"min_impurity_split\": 1e-07, \"random_state\": null, \"max_leaf_nodes\": null, \"min_samples_leaf\": 1, \"min_weight_fraction_leaf\": 0.0, \"max_features\": \"sqrt\", \"verbose\": 0, \"max_depth\": null, \"warm_start\": false, \"criterion\": \"gini\", \"n_estimators\": 5}]]\n"
     ]
    }
   ],
   "source": [
    "train_and_run_classifier(method_parameters_combinations, reference_dbs, pipelines, sweep, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate per-method biom tables\n",
    "\n",
    "Modify the taxonomy_glob below to point to the taxonomy assignments that were generated above. This may be necessary if filepaths were altered in the preceding cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "taxonomy_glob = join(results_dir, '*', '*', '*', '*', 'rep_set_tax_assignments.txt')\n",
    "generate_per_method_biom_tables(taxonomy_glob, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move result files to repository\n",
    "\n",
    "Add results to the short-read-taxa-assignment directory (e.g., to push these results to the repository or compare with other precomputed results in downstream analysis steps). The precomputed_results_dir path and methods_dirs glob below should not need to be changed unless if substantial changes were made to filepaths in the preceding cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precomputed_results_dir = join(project_dir, \"data\", \"precomputed-results\", analysis_name)\n",
    "method_dirs = glob(join(results_dir, '*', '*', '*', '*'))\n",
    "move_results_to_repository(method_dirs, precomputed_results_dir)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
